#!/usr/bin/env python3

#stdlib imports
import argparse
import configparser
import os
import urllib.request, urllib.error, urllib.parse
import json
import sqlite3 as lite
from collections import OrderedDict
from datetime import datetime
import numpy as np
import tempfile
import shutil
import gfail.pdl as pdl
import time

#third party
from impactutils.io.cmd import get_command_output
#from impactutils.transfer.emailsender import EmailSender
from impactutils.transfer.factory import get_sender_class

#dictionary containing table definitions and column definitions column:type
tablecols = [('id', 'integer primary key'),
             ('eventcode', 'text'),
             ('version', 'integer'),
             ('sent_email', 'text'),
             ('lat', 'real'),
             ('lon', 'real'),
             ('depth', 'real'),
             ('time', 'timestamp'),
             ('mag', 'real'),
             ('alert', 'text'),
             ('maxmmi', 'real'),
             ('location', 'text'),
             ('finitefault', 'integer'),
             ('url', 'text'),
             ('jsonfeed', 'text'),
             ('Hagg05LS', 'real'),
             ('Parea01LS', 'real'),
             ('Hagg05LQ', 'real'),
             ('Parea01LQ', 'real')]

TABLES = {'shakemap': OrderedDict(tablecols)}

ALERTLEVELS = ['pending', 'green', 'yellow', 'orange', 'red']

defaultThresh = {'mag': 6.5,
                 'eis': 'yellow',
                 'mmi': 7}

homedir = os.path.dirname(os.path.abspath(__file__))
testdatadir = os.path.abspath(os.path.join(os.path.dirname(homedir), 'tests', 'data'))


def main(args):
    print('%s - Running autogf' % datetime.now())

    # check files specified actually exist
    args = validateArgs(args)

    config = configparser.ConfigParser()

    gfail_flags = ''
    if args.webpage:
        gfail_flags += '-w'
    if args.hdf5:
        gfail_flags += ' --hdf5'
    if args.make_static_pngs:
        gfail_flags += ' -pn'

    if not args.webpage and not args.hdf5:
        print('No outputs specified, saving static plots')
        gfail_flags = '-pn'

    if args.autoconfig is None:
        mail = False
        print('No autoconfig.ini file, using default thresholds:\n %s' % defaultThresh)
        thresh = defaultThresh
    else:
        with open(args.autoconfig) as f:
            config.read_file(f)
        sections = config.sections()

        thresh = defaultThresh
        if 'THRESHOLDS' not in sections:
            print('autoconfig.ini missing THRESHOLDS section, using defaults:\n %s' % defaultThresh)
        else:  # Update defaults
            for key in config.options('THRESHOLDS'):
                value = config.get('THRESHOLDS', key)
                if key in ['mag', 'mmi']:
                    value = float(value)
                    thresh[key] = value
                if key in ['eis']:
                    thresh[key] = value

        if 'MAIL' not in sections:
            print('autoconfig.ini missing MAIL section, no emails will be sent')
            mail = False
        else:
            try:
                changethresh = config.get('MAIL', 'update')
                sendthresh = float(config.get('MAIL', 'stopafter'))
            except Exception as e:
                print(e)
                print('Not able to find mail filter thresholds, using defaults')
                changethresh = 0.25
                sendthresh = None
            mail = True

    if args.testrun:
        tempdir = tempfile.mkdtemp()
        gfail_flags = gfail_flags + ' -o %s' % tempdir
        dryrun = True
        mail = False
        testurl = 'https://earthquake.usgs.gov/fdsnws/event/1/query?eventid=nc216859&format=geojson'
        with urllib.request.urlopen(testurl) as fh:
            data = fh.read().decode('utf8')
        testjd = json.loads(data)
        smp = testjd['properties']['products']['shakemap'][0]
        tempevent = getProductInfo(smp)
        tempevent['time'] = datetime.now()
        tempevent['title'] = 'Test of Loma Prieta'
        tempevent['finite_fault'] = None
        tempevent['update'] = False
        tempevent['prevermax'] = None
        testrecents = [tempevent]
        tempevent['url'] = os.path.join(testdatadir, 'loma_prieta', 'grid.xml')
    else:
        dryrun = False

    if args.eventdir is None or args.testrun:
        db, cursor = connect(args.db)
        if args.testrun:
            recentevents = testrecents
        else:
            recentevents = getRecentEvents(thresh, args.feed, cursor=cursor)
        for event in recentevents:
            empty = False
            update = event['update']

            # Insert an "in progress" flag into database so a second instance doesnt start for same version
            fmt = 'INSERT INTO shakemap (eventcode,version,lat,lon,depth,time,mag,alert,maxmmi,location,finitefault,url,jsonfeed) VALUES ("%s",%i,%.4f,%.4f,%.1f,"%s",%.1f,"%s",%.1f,"%s",%i,"%s","%s")'
            eid = event['eventcode']
            enum = event['version']
            elat = event['lat']
            elon = event['lon']
            edepth = event['depth']
            etime = event['time']
            emag = event['mag']
            alert = event['alert']
            maxmmi = event['maxmmi']
            eloc = event['location']
            url = 'In progress'
            jsonfeed = 'In progress'
            finitefault = 0

            if not args.testrun:
                insertquery = fmt % (eid, enum, elat, elon, edepth,
                                     str(etime), emag, alert, maxmmi, eloc,
                                     finitefault, url, jsonfeed)
                cursor.execute(insertquery)
                db.commit()

            filenames, url, jsonfeed = runGF(gfail_flags,
                                             args.gfailconfig,
                                             event['url'],
                                             event['eventcode'],
                                             pdlconf=args.pdlconf,
                                             faultfile=event['finite_fault'],
                                             dryrun=dryrun)
            change = ''

            timediff = np.abs((etime-datetime.now()).days)  # Time between event and present date
            toolong = False
            prevermax = event['prevermax']

            if filenames is None:
                #print('No outputs found, problem with codes\n')
                empty = True
                url = 'Failed to run'
                jsonfeed = 'Failed to run'
            else:
                
                try:
                    infojsonfile = [filen for filen in filenames if 'info.json' in filen]
                    with open(infojsonfile[0], 'r') as fij:
                        data = fij.read()
                    infodict = json.loads(data)

                    LQstats = infodict['Liquefaction']['models']['Zhu and others (2017)']['stats']
                    LSstats = infodict['Landslides']['models']['Nowicki and others (2014)']['stats']
                    LQsta = [LQstats['Hagg_0.05g'], LQstats['Parea_0.10']]
                    LSsta = [LSstats['Hagg_0.05g'], LSstats['Parea_0.10']]
                    if update and mail:
                        if sendthresh is not None:
                            if timediff > sendthresh:
                                mail = False
                                toolong = True
                            else:
                                # See if stats changed enough to send an update
                                sig = False
                                # Get previous versions values

                                cursor.execute("SELECT Hagg05LS, Parea01LS, Hagg05LQ, Parea01LQ FROM shakemap WHERE eventcode = ? and version = ?",
                                               (event['eventcode'], str(prevermax)))
                                temp = cursor.fetchone()
                                Hagg05LS, Parea01LS, Hagg05LQ, Parea01LQ = temp
                                # Send mail if any changes from 0 to a value or vice versa
                                if (np.round(Hagg05LS) == 0. and np.round(LSsta[0]) > 0.) or (np.round(Hagg05LS) > 0. and np.round(LSsta[0]) == 0.):
                                    sig = True
                                    change += '\n\t* Landslide aggregate hazard (>5%%g) changed from %1.0f to %1.0f.' % (Hagg05LS, LSsta[0])
                                if (np.round(Hagg05LQ) == 0. and np.round(LQsta[0]) > 0.) or (np.round(Hagg05LQ) > 0. and np.round(LQsta[0]) == 0.):
                                    sig = True
                                    change += '\n\t* Liquefaction aggregate hazard (>5%%g) changed from %1.0f to %1.0f.' % (Hagg05LQ, LQsta[0])
                                if (np.round(Parea01LS) == 0. and np.round(LSsta[1]) > 0.) or (np.round(Parea01LS) > 0. and np.round(LSsta[1]) == 0.):
                                    sig = True
                                    change += '\n\t* Landslide area of probability >0.1 changed from %1.0f to %1.0f.' % (Parea01LS, LSsta[1])
                                if (np.round(Parea01LQ) == 0. and np.round(LQsta[1]) > 0.) or (np.round(Parea01LQ) > 0. and np.round(LQsta[1]) == 0.):
                                    sig = True
                                    change += '\n\t* Liquefaction area of probability >0.1 changed from %1.0f to %1.0f.' % (Parea01LQ, LQsta[1])
                                if not sig:
                                    changeperc = []
                                    changereason = []
                                    changeperc.append(np.abs(Hagg05LS-LSsta[0])/Hagg05LS)
                                    changereason.append('\n\t* Landslide aggregate hazard (>5%%g) changed from %1.0f to %1.0f.' % (Hagg05LS, LSsta[0]))
                                    changeperc.append(np.abs(Hagg05LQ-LQsta[0])/Hagg05LQ)
                                    changereason.append('\n]t* Liquefaction aggregate hazard (>5%%g) changed from %1.0f to %1.0f.' % (Hagg05LQ, LQsta[0]))
                                    changeperc.append(np.abs(Parea01LS-LSsta[1])/Parea01LS)
                                    changereason.append('\n]t* Landslide area of probability >0.1 changed from %1.0f to %1.0f.' % (Parea01LS, LSsta[1]))
                                    changeperc.append(np.abs(Parea01LQ-LQsta[1])/Parea01LQ)
                                    changereason.append('\n]t* Liquefaction area of probability >0.1 changed from %1.0f to %1.0f.' % (Parea01LQ, LQsta[1]))
                                    for perc, reas in zip(changeperc, changereason):
                                        if perc >= changethresh:
                                            sig = True
                                            change += reas
                                if not sig:
                                    mail = False

                except Exception as e:
                    print(e)
                    LQstats = [None, None]
                    LSstats = [None, None]

            utctime = time.strftime('%Y-%m-%d %H:%M:%S UTC', time.gmtime())
            if mail and not empty:
                mailUsers(event, args.autoconfig, url, jsonfeed, update, change)

                cursor.execute("UPDATE shakemap SET sent_email = ? WHERE eventcode = ? AND version =?",
                               (utctime, event['eventcode'], event['version']))
                db.commit()
            elif toolong and not empty:
                cursor.execute("UPDATE shakemap SET sent_email = ? WHERE eventcode = ? AND version =?",
                               ('too long after event. %s' % utctime, event['eventcode'], event['version']))
                db.commit()
            elif not empty:
                cursor.execute("UPDATE shakemap SET sent_email = ? WHERE eventcode = ? AND version =?",
                               ('not a significant change. %s' % utctime, event['eventcode'], event['version']))
                db.commit()
            else:
                cursor.execute("UPDATE shakemap SET sent_email = ? WHERE eventcode = ? AND version =?",
                               ('no files returned. %s' % utctime, event['eventcode'], event['version']))
                db.commit()


            if event['finite_fault'] is not None:
                finitefault = 1
            else:
                finitefault = 0
            if not args.testrun:
                # Update url and jsonfeed
                cursor.execute("UPDATE shakemap SET jsonfeed = ?, url = ?, finitefault=? WHERE eventcode = ? AND version =?",
                               (jsonfeed, url, finitefault, event['eventcode'], event['version']))
                db.commit()
                cursor.execute("UPDATE shakemap SET Hagg05LS = ?, Hagg05LQ = ?, Parea01LS = ?, Parea01LQ = ? WHERE eventcode = ? AND version =?",
                               (LSsta[0], LQsta[0], LSsta[1], LQsta[1], event['eventcode'], event['version']))
                db.commit()

            # Clean up finite fault temp geojson files
            if finitefault == 1:
                os.remove(event['finite_fault'])

    if args.eventdir is not None or (args.testrun and args.webpage):  # Also test this functionality
        # Just send the files
        if args.testrun:
            event_id = '19891018000415'
            eventdir = os.path.join(tempdir, event_id)
            dryrun = True
        else:
            eventdir = args.eventdir
            event_id = os.path.basename(args.eventdir)
        success, url, feed = gf_transfer(eventdir, event_id, args.pdlconf, dry_run=dryrun)
        if success and not dryrun:
            print('Folder for event %s successfully sent, viewable at: %s' % (event_id, url))
        elif success and dryrun:
            print('Dry run for event %s successful' % (event_id,))
        else:
            raise Exception('Folder was not transfered')

    if args.testrun:
        # This was a test run so delete files and remove entries from database
        print('Test successful, cleaning up files')
        shutil.rmtree(tempdir)


def validateArgs(args):
    if args.pdlconf is not None and not os.path.exists(args.pdlconf):
        print('Pdl config file specified (%s) does not exist, files will not be transferred' % args.pdlconf)
        args.pdlconf = None

    if args.eventdir is not None:
        if not os.path.exists(args.eventdir):
            print('event directory does not exist, rerunning event')
            args.eventdir = None
        else:
            return args  # don't need to check the rest of them if just sending files

    if not os.path.exists(args.gfailconfig):
        raise IOError('gfail config file specified (%s) does not exist' % args.gfailconfig)
    if args.autoconfig is not None:
        if not os.path.exists(args.autoconfig):
            raise IOError('autoconfig file specified (%s) does not exist' % args.autoconfig)
    if not os.path.exists(args.db):
        print('database file specified (%s) does not yet exist, new one will be created' % args.db)
    if args.feed not in 'dayweekmonth':
        print('Feed specified not valid, using daily feed')
        args.feed = 'http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5_day.geojson'
    elif args.feed in 'month':
        args.feed = 'http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5_month.geojson'
    elif args.feed in 'week':
        args.feed = 'http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5_week.geojson'
    else:
        args.feed = 'http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5_day.geojson'

    return args


def mailUsers(event, autoconfig, url, jsonfeed, update=False, change=''):
    config = configparser.ConfigParser()
    with open(autoconfig) as f:
        config.read_file(f)

    props = {}
    eid = event['eventcode']
    title = event['title']
    vnum = event['version']

    if update:
        props['subject'] = 'UPDATE to Ground Failure Model Results for %s' % (title)
        props['message'] = """
        Significant changes to Ground Failure models for ShakeMap v%i of %s (event id %s):\n%s
        
        Model summary page: %s

        Event detail JSON feed: %s
        
        You will only receive notification of updates if model outputs change significantly.
        For minor updates, find most recent results by checking the event detail json feed
        for an updated link.

        Model results are provisional and subject to revision. The results are released
        on the condition that neither the USGS nor the U.S. Government may be held liable
        for any damages resulting from its authorized or unauthorized use. As this
        product is still in development, do not forward the links or share widely
        without permission.

        Do not reply to this message. Contact kallstadt@usgs.gov with questions.
        """ % (vnum, title, eid, change, url, jsonfeed)
    else:
        props['subject'] = 'Ground Failure Model Results for %s' % (title)
        props['message'] = """
        Ground Failure models for ShakeMap v%i of %s (event id %s).

        You will only receive notification of updates if model outputs change significantly,
        for minor updates, find most recent results by checking the event detail json feed below.

        Model summary page: %s

        Event detail JSON feed: %s

        Model results are provisional and subject to revision. The results are released
        on the condition that neither the USGS nor the U.S. Government may be held liable
        for any damages resulting from its authorized or unauthorized use. As this
        product is still in development, do not forward the links or share widely
        without permission.

        Do not reply to this message. Contact kallstadt@usgs.gov with questions.
        """ % (vnum, title, eid, url, jsonfeed)

    props['sender'] = config.get('MAIL', 'sender')
    props['recipients'] = config.get('MAIL', 'recipients').split(',')
    props['smtp_servers'] = [config.get('MAIL', 'server')]
    #props['zip_file'] = '%s.zip' % eid
    sender = get_sender_class(config.get('MAIL', 'stype'))
    sender1 = sender(properties=props)
    sender1.send()


def runGF(flagstr, modelconfig, shakefile, event_id, pdlconf=None,
          transfer=True, faultfile=None, dryrun=False):
    if dryrun:
        dryrun = '-d'
    else:
        dryrun = ''
    if faultfile is not None:
        cmd = 'gfail -f %s %s %s %s' % (faultfile, flagstr, modelconfig, shakefile)
    else:
        cmd = 'gfail %s %s %s' % (flagstr, modelconfig, shakefile)
    print(cmd)
    retcode, stdout, stderr = get_command_output(cmd)
    temp = stdout.decode('utf-8')
    if temp.find('Files created:\n') > -1:
        temp = temp.split('Files created:\n')[1]
        filenames = []
        for line in temp.split('\n'):
            if 'pdf' in line or 'png' in line or 'html' in line or 'hdf5' in line or 'bil' in line or 'webpage' or 'json':
                filenames.append(line)
    else:
        print("""
            Did not find any files output by runGF for event_id %s,
            Continuing to next event\n \
            Warnings that were output: \
            %s\n""" % (event_id, stderr.decode()))
        filenames = None
        url = None
        jsonfeed = None
        return filenames, url, jsonfeed

    event_dir = os.path.dirname([fn for fn in filenames if '.bil' in fn or 'webpage' in fn or '.png' in fn][0])

    if transfer and pdlconf is not None:
        success, url, jsonfeed = gf_transfer(event_dir, event_id, pdlconf, dryrun)
        if not success:
            print('PDL transfer failed')
            url = None
            jsonfeed = None
    else:
        url = None
        jsonfeed = None

    return filenames, url, jsonfeed


def getProductInfo(shakemap, pager=None):
    edict = {}
    edict['eventcode'] = shakemap['code']
    edict['version'] = int(shakemap['properties']['version'])
    edict['lat'] = float(shakemap['properties']['latitude'])
    edict['lon'] = float(shakemap['properties']['longitude'])
    edict['depth'] = float(shakemap['properties']['depth'])
    #shakemap properties don't have event time, fill this in from event info
    edict['mag'] = float(shakemap['properties']['magnitude'])
    edict['location'] = shakemap['properties']['event-description']
    edict['url'] = shakemap['contents']['download/grid.xml']['url']
    if pager is not None:
        edict['alert'] = pager['properties']['alertlevel']
    else:
        edict['alert'] = None
    try:
        edict['maxmmi'] = float(shakemap['properties']['maxmmi'])
    except:
        edict['maxmmi'] = float(pager['properties']['maxmmi'])
    return edict


def getRecentEvents(thresholds, feed, cursor=None):
    """Finds recent events above thresholds.
    If cursor is connection to database, this will also search for any events
    that were previously run and have updated ShakeMaps that no longer exceed
    thresholds.
    """
    fh = urllib.request.urlopen(feed)
    data = fh.read().decode('utf8')
    jdict = json.loads(data)
    fh.close()
    eventlist = []
    for event in jdict['features']:

        etypes = event['properties']['types'].strip(',').split(',')
        if 'shakemap' not in etypes:
            continue
        if 'losspager' not in etypes:
            PAG = False
        else:
            PAG = True

        edict = {}
        detailurl = event['properties']['detail']
        fh = urllib.request.urlopen(detailurl)
        data = fh.read().decode('utf8')
        jdict2 = json.loads(data)
        fh.close()
        shakemap = jdict2['properties']['products']['shakemap'][0]
        
        if PAG:
            try:
                pager = jdict2['properties']['products']['losspager'][0]
            except:
                print('No losspager product found, setting pager to None\n')
                pager = None
                PAG = False
        else:
            pager = None

        edict = getProductInfo(shakemap, pager)
        
        # First check if any are updates to events already in the database
        update = False
        prevermax = None
        if cursor is not None:
            fmt = 'SELECT id FROM shakemap WHERE eventcode="%s"'
            query = fmt % (edict['eventcode'])
            cursor.execute(query)
            row = cursor.fetchone()
            if row is not None:
                cursor.execute("SELECT version FROM shakemap WHERE eventcode = ?", (edict['eventcode'],))
                row2 = cursor.fetchall()
                if len(row2) > 0:
                    prevers = [r8[0] for r8 in row2]
                    prevermax = np.max(prevers)
                    if prevermax < edict['version']:
                        update = True
                    else:
                        # if previous version is equal to or greater than current version, don't want to run, skip to next event
                        continue

        #check shakemap thresholds
        pmag = float(shakemap['properties']['magnitude'])
        pmmi = float(shakemap['properties']['maxmmi'])
        if PAG:
            palert = ALERTLEVELS.index(pager['properties']['alertlevel'])
        getShake = False
        if 'mag' in thresholds and pmag > thresholds['mag']:
            getShake = True
        if 'mmi' in thresholds and pmmi > thresholds['mmi']:
            getShake = True
        if PAG:
            if 'eis' in thresholds and palert >= ALERTLEVELS.index(thresholds['eis']):
                getShake = True

        if getShake or update:
            
            # search for finitefault presence
            ff = False
            finft = None
            for key, value in shakemap['contents'].items():
                if '_fault.txt' in key:
                    ff = True
                    d = text_to_json(value['url'])
                elif 'rupture.json' in key:
                    ff = True
                    with urllib.request.urlopen(value['url']) as f2:
                        d = f2.read()
            if ff:
                with tempfile.NamedTemporaryFile(delete=False, mode='w') as f3:
                    f3.write(d)
                    finft = f3.name  # save name of geojson file
                    #print(f3.name)
            
            edict['time'] = datetime.utcfromtimestamp(event['properties']['time']/1000)
            edict['title'] = event['properties']['title']
            edict['finite_fault'] = finft
            edict['update'] = update
            edict['prevermax'] = prevermax
            eventlist.append(edict.copy())

    return eventlist


def connect(dbfile):
    doCreate = False
    if not os.path.isfile(dbfile):
        doCreate = True

    db = lite.connect(dbfile)
    cursor = db.cursor()
    if doCreate:
        for tablename, table in TABLES.items():
            querynuggets = []
            for key, value in table.items():
                nugget = '%s %s' % (key, value)
                querynuggets.append(nugget)
            query = 'CREATE TABLE %s (%s)' % (tablename, ', '.join(querynuggets))
            cursor.execute(query)
            db.commit()

    return (db, cursor)


def text_to_json(url):
    """
    Simplification of text_to_json from shakelib.rupture.factory
    """
    with urllib.request.urlopen(url) as f:
        lines = f.readlines()
    x = []
    y = []
    z = []
    reference = ''
    # convert to geojson
    for line in lines:
        sline = line.decode('utf8').strip()
        if sline.startswith('#'):
            reference += sline.strip('#').strip('Source: ')
            continue
        if sline.startswith('>'):
            if len(x):  # start of new line segment
                x.append(np.nan)
                y.append(np.nan)
                z.append(np.nan)
                continue
            else:  # start of file
                continue
        if not len(sline.strip()):
            continue
        parts = sline.split()

        y.append(float(parts[0]))
        x.append(float(parts[1]))
        if len(parts) >= 3:
            z.append(float(parts[2]))
        else:
            print('Fault file has no depths, assuming zero depth')
            z.append(0.0)
        coords = []
        poly = []
        for lon, lat, dep in zip(x, y, z):
            if np.isnan(lon):
                coords.append(poly)
                poly = []
            else:
                poly.append([lon, lat, dep])
        if poly != []:
            coords.append(poly)

    d = {
        "type": "FeatureCollection",
        "metadata": {
            'reference': reference
        },
        "features": [
            {
                "type": "Feature",
                "properties": {
                    "rupture type": "rupture extent"
                },
                "geometry": {
                    "type": "MultiPolygon",
                    "coordinates": [coords]
                }
            }
        ]
    }
    return json.dumps(d)


def gf_transfer(eventdir, event_id, pdl_config=None, dry_run=False):
    """
    Transfer ground failure results to dev server
    """
    print('Preparing directory to transfer %s...'
          % event_id)
    pdl.prepare_pdl_directory(eventdir)

    if pdl_config is None:
        print('PDL directory prepared, no pdl_config provided so no files were sent')
        return (True, None, None)
    else:
        # Transfer
        print('Transferring...')
        log = pdl.transfer(eventdir, pdl_config, dryrun=dry_run)
        print('done.')
        if log['rc'] is True:
            print('Successful PDL transfer.')
            success = True
    
            # Construct URL to index.html
            url_template = ('https://dev01-earthquake.cr.usgs.gov/archive'
                            '/product/[TYPE]/[ID]/[SOURCE]/[TIME]/html/index.html')
            se_lines = log['se'].decode().split("\n")
            info_line = [l for l in se_lines if 'send complete Socket' in l][0]
            info = info_line.split(':')
            timestamp = info[-1].replace("'", "")
            eventid = info[-2]
            ptype = info[-3]
            source = info[-4]
            url = url_template.replace(
                '[TYPE]', ptype).replace(
                '[ID]', eventid).replace(
                '[SOURCE]', source).replace(
                '[TIME]', timestamp)
    
            # URL for event's detail feed
            feed = ('https://dev01-earthquake.cr.usgs.gov/fdsnws/event/1/'
                    'query?eventid=[EVENTID]&format=geojson')
            feed = feed.replace('[EVENTID]', eventid)
    
            # Print to screen
            print("Web page URL:")
            print(url)
            print("Event detail JSON feed:")
            print(feed)
    
            # Save to file
            #with open("url.txt", "w") as f:
            #    f.write(url)
            #    f.write(feed)
        else:
            print('PDL transfer failed.')
            print(log['so'].decode())
            success = False
    
        return success, url, feed
        

if __name__ == '__main__':

    default_loc = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

    parser = argparse.ArgumentParser(
        description="""Check for triggering events for ground failure, if found,
        run gfail and transfer to dev comcat. Note that defaults for gfail
        should be set prior to running this code""")

    parser.add_argument(
        '-c', '--gfailconfig', metavar='gfailconfig', nargs='?',
        default=os.path.join(default_loc, 'configlist.txt'),
        help='single config file of model to run (.ini extension), or text '
             'file listing config files (do not use .ini extension)')

    parser.add_argument(
        '-a', '--autoconfig', metavar='autoconfig',
        nargs='?', default=None,
        help="""
        Location of autoconfig.ini file that contains threshold
        information and email address list for automated emails.
        If not specified, will not send emails and will use default
        thresholds""")

    parser.add_argument(
        '-d', '--db', metavar='sqldatabase', nargs='?',
        default=os.path.join(default_loc, 'events_run.db'),
        help='SQLite database file location that stores history'
             'of ground failure runs')

    parser.add_argument(
        '-e', '--eventdir', metavar='path to folder containing output',
        nargs='?', default=None, help="""
        Send results that were already computed. eventdirectory should be
        to file path to the directory containing all the files (usually a
        subfolder named with the event id)
        """)
    parser.add_argument(
        '-p', '--pdlconf', metavar='pdlconfig', nargs='?',
        default=None,
        help="""
        Location of pdl config file on local machine, if not specified
        will do a dry run""")

    parser.add_argument(
        '--feed', metavar='jsonfeed', nargs='?',
        default='day', help='Which jsonfeed to check, default day, other'
                            'options are week and hour')

    # Binary
    parser.add_argument(
        '-t', '--testrun', action='store_true', default=False,
        help='Do a test run, will not transfer files (dry run)'
             'and will delete all files created')
             
    parser.add_argument(
        '-w', '--webpage', action='store_true', default=False,
        help='Make web page')

    parser.add_argument(
        '--hdf5', action='store_true', default=False,
        help='Save hdf5 files')
    
    parser.add_argument(
        '-pn', '--make-static-pngs', action='store_true', default=False,
        help='Make static plots for each model')

    pargs = parser.parse_args()
    main(pargs)
