#!/usr/bin/env python

import matplotlib
matplotlib.use('Agg')

# stdlib imports
import argparse
import os.path
import sys
import glob
from datetime import datetime
import logging
import logging.config
import sqlite3
from collections import OrderedDict
import json
import re

# third party imports
from impactutils.io.cmd import get_command_output
from impactutils.comcat.query import GeoServe
from mapio.shake import getHeaderData
from mapio.shake import ShakeGrid
import configobj
import fiona
from shapely.geometry import shape, box
from mpl_toolkits.basemap import Basemap

# local imports
from gfail.gfailrun import run_gfail
from gfail.transfer import gf_transfer

# config parameters required for this program to run
REQUIRED_CONFIG = ['log_filepath', 'output_filepath',
                   'trimfile', 'dbfile', 'pdl_config']

# what are the networks from which we will accept ShakeMap products?
WHITELIST = ['us', 'ci', 'nc', 'nn', 'hv', 'uw', 'nn', 'uu', 'ak']

# what are the networks that have origins but not ShakeMaps
NO_SHAKEMAPS = ['mb', 'ecx', 'tul', 'ismp',
                'nm', 'se', 'ogso', 'pr', 'neic', 'ld', 'wy']

# minimum magnitude to be processed
MAG_THRESHOLD = 4.0

# logging configuration - sets up a rotating log file
LOG_CFG = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'standard': {
            'format': ('%(levelname)s -- %(asctime)s -- '
                       '%(module)s.%(funcName)s -- %(message)s'),
            'datefmt': '%Y-%m-%d %H:%M:%S',
        },
    },
    'handlers': {
        'default': {
            'level': 'INFO',
            'formatter': 'standard',
            'class': 'logging.handlers.TimedRotatingFileHandler',
            'when': 'midnight',
        },
    },
    'loggers': {
        '': {
            'handlers': ['default'],
            'level': 'INFO',
            'propagate': False
        },
    }
}

# name of the logfile, will get archived at midnight
LOGFILE = 'groundfailure.log'

# name of the global configuration file
CONFIG_FILE = '.gfail_defaults'

# string time format to stuff in the database
TIMEFMT = '%Y-%m-%d %H:%M:%S'

# schema definition of the event table in the sqlite database
EVENT = OrderedDict([('id', 'INTEGER PRIMARY KEY'),
                     ('eventcode', 'TEXT'),
                     ('eventdir', 'TEXT'),
                     ('version', 'INTEGER'),
                     ('lat', 'REAL'),
                     ('lon', 'REAL'),
                     ('depth', 'REAL'),
                     ('time', 'TIMESTAMP'),
                     ('mag', 'REAL'),
                     ('location', 'TEXT'),
                     ('starttime', 'TIMESTAMP'),
                     ('endtime', 'TIMESTAMP'),
                     ('finitefault', 'INTEGER'),
                     ('HaggLS', 'REAL'),
                     ('ExpPopLS', 'REAL'),
                     ('HaggLQ', 'REAL'),
                     ('ExpPopLQ', 'REAL')])


def get_next_version(eventdir):
    """Get the number for the next version for the input event directory.

    Args:
        eventdir (str): Path to event directory.

    Returns:
        int - Number of next version of groundfailure product.

    """
    vfolders = glob.glob(os.path.join(eventdir, 'version.*'))
    if not len(vfolders):
        return 1
    vfolders = sorted(vfolders)
    old_version = int(re.search('[0-9]+', vfolders[-1]).group())
    version = old_version + 1
    return version


def shakemap_over_land(landfile, grid):
    """Test to see if any portion of the ShakeMap grid is over land.

    Args:
        landfile (str): Path to global shapefile of land masses.
        grid (dict): grid containing fields:
                     - lon_min
                     - lon_max
                     - lat_min
                     - lat_max
    Returns:
        bool: True if over land, False if not.
    """
    # Check if ShakeMap is entirely in water
    on_land = False

    box_tuple = (grid['lon_min'], grid['lat_min'],
                 grid['lon_max'], grid['lat_max'])
    grid_box = box(*box_tuple)

    with fiona.open(landfile, 'r') as shapefile:
        tfeatures = shapefile.items(bbox=box_tuple)
        features = [shape(feature[1]["geometry"]) for feature in tfeatures]

    
    for feature in features:
        if feature.intersects(grid_box):
            on_land = True
            break

    return on_land


def connect_database(dbfile):
    """Connect to the sqlite database, create if necessary.

    Args:
        dbfile (str): Path to sqlite database file.
    Returns:
        connection: SQLITE connection object.
        cursor: SQLITE cursor object.
    """

    # Create the database if it doesn't exist
    db_exists = os.path.isfile(dbfile)
    connection = None
    cursor = None
    connection = sqlite3.connect(dbfile)
    cursor = connection.cursor()
    if not db_exists:
        createcmd = 'CREATE TABLE shakemap ('
        nuggets = []
        for column, ctype in EVENT.items():
            nuggets.append('%s %s' % (column, ctype))
        createcmd += ','.join(nuggets) + ')'
        cursor.execute(createcmd)

    return connection, cursor


def get_version_dir(eventid, eventdir, config):
    """Get the version directory for an event.

    Args:
        eventid (str): Event ID.
        eventdir (str): Path to (possibly non-existent) event directory.
        config (ConfigObj): Configuration object.

    Returns:
        str: Path to version dictionary.
        int: Version number.
        str: Event directory.
    """
    # if eventdir is None, create a new one
    if eventdir is None:
        tnow = datetime.utcnow().strftime('%Y%m%d%H%M%S')
        eventdir = os.path.join(
            config['output_filepath'], '%s_%s' % (eventid, tnow))
        if os.path.isdir(eventdir):  # how did this happen??
            version = get_next_version(eventdir)
        else:
            os.makedirs(eventdir)
            version = 1
    else:
        if not os.path.isdir(eventdir):  # this shouldn't happen either...
            os.makedirs(eventdir)
            version = 1
        else:
            version = get_next_version(eventdir)

    # create a version directory
    vdir = os.path.join(eventdir, 'version.%03i' % version)
    return (vdir, version, eventdir)


def get_event_dir(args, cursor):
    """Scan the database for existing event directory.

    Args:
        args (arparser Namespace): Input arguments.
        cursor (sqlite cursor): Database cursor object.

    Returns:
        int: Database ID of event (-1 if not found).
        str: Event directory (None if not found).
    """
    # get all the event IDS associated with this event, then we can scan the
    # database to see if we have processed this event before.
    eventdir = None
    db_id = -1
    eventids = args.eventids.split(',')
    for eventid in eventids:
        idquery = ('SELECT id,eventdir FROM shakemap WHERE eventcode="%s"' %
                   eventid)
        cursor.execute(idquery)
        row = cursor.fetchone()
        if row is None:
            continue
        db_id, eventdir = row
        break

    return (db_id, eventdir)


def insert_event(connection, cursor, eventid, event, version, eventdir):
    """Insert known event information into database.

    Args:
        connection (sqlite connection): connection object.
        cursor (sqlite cursor): cursor object.
        eventid (str): Event ID.
        event (dict): Event dictionary.
        version (int): Groundfailure version number.
        eventdir (str): Path to groundfailure event directory.
    Returns:
        int: Database ID of event row inserted.
    """
    #cnames = ['eventcode', 'eventdir', 'version',
    #          'lat', 'lon', 'depth',
    #          'time', 'mag', 'location', 'starttime']
    #columns = ','.join(cnames)
    #vfmt = '("%s","%s",%i,%.4f,%.4f,%.1f,"%s",%.1f,"%s","%s")'
    vfmt = ("%s", "%s", "%i", "%.4f", "%.4f", "%.1f", "%s", "%.1f", "%s", "%s")   
    tnowstr = datetime.utcnow().strftime(TIMEFMT)
    tpl = (eventid, eventdir, version,
           event['lat'], event['lon'],
           event['depth'], event['event_timestamp'].strftime(TIMEFMT),
           event['magnitude'], event['event_description'], tnowstr)
    list1 = []
    for fm, tp in zip(vfmt, tpl):
        list1.append(fm % tp)
    cursor.execute("""INSERT INTO shakemap (eventcode, eventdir, version, lat,
                   lon, depth, time, mag, location, starttime)
                   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)""",
                   (list1))

    db_id = cursor.lastrowid
    connection.commit()
    return db_id


def process_shakemap(args, config):
    """Process the ShakeMap.

    Args:
        args (arparser Namespace): Input arguments.
        config (ConfigObj): Configuration object.
    """
    logging.info('###### Event source: %s' % args.preferred_eventsource)
    logging.info('###### Event source code: %s' %
                 args.preferred_eventsourcecode)
    logging.info('args: \n%s' % str(args))

    if args.status == 'DELETE':
        # look at this with respect to archiving and sending cancel messages
        msg = 'No action to take with delete messages.'
        logging.info(msg)
        if not args.force:
            sys.exit(1)

    logging.info('Checking if status is update...')
    if args.status != 'UPDATE':
        msg = 'No action to take with %s messages.' % args.status
        logging.info(msg)
        if not args.force:
            sys.exit(1)

    logging.info('Checking action...')
    if args.action not in ('EVENT_ADDED', 'EVENT_UPDATED',
                           'PRODUCT_ADDED', 'PRODUCT_UPDATED'):
        msg = 'No action to take with %s messages.' % args.action
        logging.info(msg)
        if not args.force:
            sys.exit(1)

    logging.info('Checking gridfile...')
    gridfile = os.path.join(args.directory, 'download', 'grid.xml')
    if not os.path.isfile(gridfile):
        gridfile = os.path.join(args.directory, 'grid.xml')
        if not os.path.isfile(gridfile):
            msg = ('Could not find input ShakeMap grid file at %s. Exiting.'
                   % gridfile)
            logging.info(msg)
            if not args.force:
                sys.exit(1)

    logging.info('Checking if Shakemap extends outside of latitude range of '
                 'slope data ...')
    shake_bounds = ShakeGrid.load(gridfile, adjust='res').getBounds()
    lat_min = shake_bounds[2]
    lat_max = shake_bounds[3]
    if (lat_min < -56.0000) or (lat_max > 84.0000):
        msg = ('Shakemap extends outside of latitude range of slope data. '
               'Existing.')
        logging.info(msg)
        if not args.force:
            sys.exit(1)

    logging.info('Checking source...')
    if args.preferred_eventsource not in WHITELIST:
        msg = ('Input ShakeMaps must be from an approved list of sources: %s. '
               'Exiting.' % str(WHITELIST))
        logging.info(msg)
        if not args.force:
            sys.exit(1)

    logging.info('Checking magnitude...')
    if args.magnitude < MAG_THRESHOLD:
        msg = ('Input ShakeMaps must be larger than M%.1f. Exiting.'
               % MAG_THRESHOLD)
        logging.info(msg)
        if not args.force:
            sys.exit(1)

    # Use lat,lon and source to exclude ShakeMap providers who are playing
    # outside their sandbox
    logging.info('Checking for authoritativeness...')

    # we'll default to letting the event run if we can't figure out who's
    # authoritative
    authtype = 'anss'
    authsrc = args.preferred_eventsource
    try:
        gs = GeoServe(args.preferred_latitude, args.preferred_longitude)
        authsrc, authtype = gs.getAuthoritative()
    except Exception as e:
        logging.info('Unable to connect to GeoServe, assuming "%s" is '
                     'authoritative.' % args.preferred_eventsource)
    if authtype != 'anss':
        authsrc = 'us'
    is_authoritative = authsrc.lower() == args.preferred_eventsource.lower()
    if not is_authoritative and authsrc.lower() not in NO_SHAKEMAPS:
        msg = ('Source %s is not authoritative for this region (%s is). '
               'Exiting.' % (args.preferred_eventsource, authsrc))
        logging.info(msg)
        if not args.force:
            sys.exit(1)

    logging.info('Checking to see if ShakeMap is over any land...')
    landfile = config['trimfile']
    hdrtuple = getHeaderData(gridfile)
    grid = hdrtuple[2]
    event = hdrtuple[1]
    if not shakemap_over_land(landfile, grid):
        msg = 'Input ShakeMap is completely over water. Exiting.'
        logging.info(msg)
        if not args.force:
            sys.exit(1)

    # connect to the database, creating it if necessary
    dbfile = config['dbfile']
    conn, cursor = connect_database(dbfile)

    # get the database ID and directory of the event we have
    # db_id = -1 and eventdir is None if event not in database
    db_id, eventdir = get_event_dir(args, cursor)

    # get the event ID for this event
    eventsource = args.preferred_eventsource
    eventcode = args.preferred_eventsourcecode
    eventid = eventsource + eventcode

    # Get the directory for this new version
    vdir, version, eventdir = get_version_dir(eventid, eventdir, config)
    os.makedirs(vdir)

    # insert what we know about the event into the database
    db_id = insert_event(conn, cursor, eventid, event,
                         version, eventdir)

    # ----------------------------------------------------------------------
    # We've passed all the filters, so call gfail
    # ----------------------------------------------------------------------

    # File containing list of model configs to include
    model_file = os.path.join(
        config['data_path'], 'autogf_models')

    logging.info("Starting gfail...")
    pargs = {'hdf5': True,
             'gis': True,
             'make_webpage': True,
             'trimfile': landfile,
             'set_default_paths': False,
             'list_default_paths': False,
             'reset_default_paths': False,
             'shakefile': gridfile,
             'make_static_pngs': False,
             'make_static_pdfs': False,
             'make_interactive_plots': False,
             'nest_folder': False,
             'config': model_file,
             'set_bounds': None,
             'make_summary': False,
             'finite_fault': None,
             'uncertfile': None,
             'save_inputs': False,
             'std': 1.0,
             'appendname': None,
             'data_path': config['data_path'],
             'output_filepath': vdir,
             'config_filepath': config['config_filepath'],
             'mapconfig': config['mapconfig'],
             'mapdata_filepath': config['mapdata_filepath'],
             'web_template': config['web_template'],
             'popfile': config['popfile'],
             'dry_run': args.dry_run,
             }

    try:
        outfiles = run_gfail(pargs)
        if outfiles is None:
            logging.info("run_gfail failed.")
            sys.exit(1)
    except Exception as e:
        logging.critical('Failure running run_gfail... "%s"' % (str(e)))
        sys.exit(1)

    # Transfer
    logging.info("Starting gfail_transfer...")
    pdl_conf_file = config['pdl_config']
    dry = ""
    if args.dry_run:
        dry = True
    success = gf_transfer(vdir, pdl_conf_file, dry_run=args.dry_run)
    if not success:
        logging.info("gf_transfer failed.")
        sys.exit(1)

    # Enter model results into the database.
    jsonfile = os.path.join(vdir, 'info.json')
    infodata = json.load(open(jsonfile, 'rt'))
    landslides = infodata['Landslides']
    landslide = None
    for landslide in landslides:
        if landslide['preferred']:
            break
    ls_hazard_value = landslide['hazard_alert']['value']
    ls_pop_value = landslide['population_alert']['value']
    liquefactions = infodata['Liquefaction']
    liquefaction = None
    for liquefaction in liquefactions:
        if liquefaction['preferred']:
            break

    lq_hazard_value = liquefaction['hazard_alert']['value']
    lq_pop_value = liquefaction['population_alert']['value']
    values = '(endtime,finitefault,HaggLS,ExpPopLS,HaggLQ,ExpPopLQ)'
    tnowstr = datetime.utcnow().strftime(TIMEFMT)
    finite_fault = not infodata['Summary']['point_source']
    query = '''UPDATE shakemap SET 
               endtime = "%s",
               finitefault = %i,
               HaggLS = %.3f,
               ExpPopLS = %.3f,
               HaggLQ = %.3f,
               ExpPopLQ = %.3f
               WHERE id = %i

''' % (tnowstr, finite_fault,
       ls_hazard_value, ls_pop_value,
       lq_hazard_value, lq_pop_value, db_id)

    # this is a little trick to get rid of extra whitespace created
    # by multiline string above
    query = ' '.join(query.split())
    cursor.execute(query)
    conn.commit()

    # close the database
    cursor.close()
    conn.close()


def main(args, config):
    """Main entry point method.

    Args:
        args (arparser Namespace): Input arguments.
        config (ConfigObj): Configuration object.

    """
    # set up a daily rotating file handler logger
    log_path = config['log_filepath']
    if not os.path.isdir(log_path):
        os.makedirs(log_path)
    logfile = os.path.join(log_path, LOGFILE)
    log_cfg = LOG_CFG.copy()
    if args.debug:
        log_cfg['handlers']['default']['class'] = 'logging.StreamHandler'
        del log_cfg['handlers']['default']['when']
    else:
        log_cfg['handlers']['default']['filename'] = logfile
    logging.config.dictConfig(log_cfg)
    logger = logging.getLogger()

    if args.type == 'shakemap':
        process_shakemap(args, config)
    else:
        logging.info('Incorrect type specified, exiting.')
    sys.exit(0)


if __name__ == '__main__':
    desc = """Call the groundfailure program with arguments from PDL.

This program is meant to be called by a PDL process, and generally not called
by a user, unless that user is a developer debugging callgf itself.

This program assumes that there is a configuration file in the user's home
directory called .gfail_defaults, which contains at least the following
entries:

log_filepath = A directory where rotating log files will be written.
output_filepath = A directory where event sub-directories will be created.
trimfile = Path to a shapefile containing GADM shapefile of country landmasses.
dbfile = Path to a SQLITE file containing event and version run information.
data_path = Path to model input data

A file called "autogf_models" that lists the models to run must be placed in the 
data_path directory.
"""
    argparser = argparse.ArgumentParser(
        description=desc,
        formatter_class=argparse.RawDescriptionHelpFormatter)
    argparser.add_argument("--directory",
                           help="Directory where ShakeMap data can be found",
                           metavar='DIRECTORY')
    argparser.add_argument("--type",
                           help="Product type", metavar='TYPE')
    argparser.add_argument("--preferred-eventsourcecode",
                           help="Product code", metavar='CODE')
    argparser.add_argument("--preferred-eventsource",
                           help="Product source", metavar='SOURCE')
    argparser.add_argument("--status",
                           help="Product status", metavar='STATUS')
    argparser.add_argument('-d', '--debug', action='store_true',
                           default=False,
                           help='Print log messages to the screen.')
    argparser.add_argument("--action",
                           help="Product action", metavar='ACTION')
    argparser.add_argument("--preferred-latitude", type=float,
                           help="Event latitude")
    argparser.add_argument("--preferred-longitude", type=float,
                           help="Event longitude", )
    argparser.add_argument("--preferred-depth", type=float,
                           help="Event depth")
    argparser.add_argument("--preferred-magnitude", type=float,
                           help="Event magnitude", metavar='MAG',
                           dest='magnitude')
    argparser.add_argument("--preferred-eventtime",
                           help="Event time", metavar='TIME', dest='time')
    argparser.add_argument("--eventids", help="List of associated event IDs")
    argparser.add_argument("--dry-run",
                           action='store_true', default=False,
                           help="Do not transfer result to PDL.")
    argparser.add_argument("-f", "--force",
                           action='store_true', default=False,
                           help="Force ground-failure run; ignore "
                                "authoritativeness checks.")

    pargs, unknown = argparser.parse_known_args()

    # make sure the config file is where we expect it to be, and read it
    config_file = os.path.join(os.path.expanduser('~'), CONFIG_FILE)
    pconfig = configobj.ConfigObj(config_file)

    if not set(REQUIRED_CONFIG).issubset(set(list(pconfig.keys()))):
        fmt = 'Missing some of the required entries in %s. Needed:'
        print(fmt % config_file)
        for req in REQUIRED_CONFIG:
            print('\t%s' % req)
        sys.exit(1)

    main(pargs, pconfig)
